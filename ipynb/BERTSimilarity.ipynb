{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERTSimilarity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNogf76gRa7TV4oCY/cWXWI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lKSSt7eTJPoa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"a3ca64c7-726b-4e5f-ae61-662afcc8ba51","executionInfo":{"status":"ok","timestamp":1591070018560,"user_tz":-540,"elapsed":18387,"user":{"displayName":"김민오","photoUrl":"","userId":"03724445089918788637"}}},"source":["# 구글 드라이브 연동, 배포시 이 코드는 주석처리함\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3Ue8PJRKW9G","colab_type":"code","colab":{}},"source":["path = \"gdrive/My Drive/gongdoli\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKobUQHPKhb3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1ce88a1c-264d-4a88-ee1e-47d4c2389e14","executionInfo":{"status":"ok","timestamp":1591070088005,"user_tz":-540,"elapsed":22037,"user":{"displayName":"김민오","photoUrl":"","userId":"03724445089918788637"}}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np  \n","import re\n","import pickle\n","\n","import keras as keras\n","from keras.models import load_model\n","from keras import backend as K\n","from keras import Input, Model\n","from keras import optimizers\n","\n","import codecs\n","from tqdm import tqdm\n","import shutil\n","\n","import warnings\n","import tensorflow as tf\n","warnings.filterwarnings(action='ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","!pip install keras-bert\n","!pip install keras-radam\n","\n","from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n","from keras_bert import Tokenizer\n","from keras_bert import AdamWarmup, calc_train_steps\n","\n","from keras_radam import RAdam"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/4e/f2/f3fe42d4a4b9baa64557749a8a4820a4f06dcb0e880a99918ddb708a07ee/keras-bert-0.82.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.4)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n","Collecting keras-transformer>=0.34.0\n","  Downloading https://files.pythonhosted.org/packages/f7/a1/78dda116b55e905df2eae292aa4321ead2771103844608d840109e3af046/keras-transformer-0.34.0.tar.gz\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.2)\n","Collecting keras-pos-embd>=0.11.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.24.0\n","  Downloading https://files.pythonhosted.org/packages/a5/f0/a9a7528b8fefacaa9c5db736036fd8c061d754830a29c34129f6847bd338/keras-multi-head-0.24.0.tar.gz\n","Collecting keras-layer-normalization>=0.14.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.7.0\n","  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n","Collecting keras-self-attention==0.41.0\n","  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.82.0-cp36-none-any.whl size=37465 sha256=373b24f2febaa13007c8130cc510a3e152f94f3676fcdd91b07237e8c7a376f0\n","  Stored in directory: /root/.cache/pip/wheels/a4/d8/62/3f2beeb2d22a2b29fc4e6fc2cede12302eb7ee82582da2dd76\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.34.0-cp36-none-any.whl size=12974 sha256=f27c1e18116ed46d74a708aa21057c21cd42514e3f2a223c8d6bcb0759ec7537\n","  Stored in directory: /root/.cache/pip/wheels/eb/41/5f/4d99dcb8312be05f4277eb64a92df0e127b94707ad26d00648\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=9ca33cee87d6057328334c3739bed45c294a5f5ea150225cc27cef4e0edd6022\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.24.0-cp36-none-any.whl size=15511 sha256=32a783fa430fe5956118e46d8d4cd5a1fed98ea583a125f09f02e47e3f3e067f\n","  Stored in directory: /root/.cache/pip/wheels/b6/84/01/dbcb50629030c8647a19dd0b7134574fad56c531bdb243bd20\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=1f45fee2f1edb8643c531830f59e6f480cf79755122b36b2b9beba1894997ccd\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=86fcb6ee6550f6f8c1472c3b63fdeeb1256594d8aa9f2e8dee0d8a35a3e728a2\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=403f103b99643bfa3f3bf97a8d77041ac2e68df774444eb9e9791ea53d948d8f\n","  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=fb02ee42fd928f6ce6cb15c96432c9952289bd8935e6bb51f5a89c58a231c12e\n","  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n","Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.82.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.24.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.34.0\n","Collecting keras-radam\n","  Downloading https://files.pythonhosted.org/packages/46/8d/b83ccaa94253fbc920b21981f038393041d92236bb541751b98a66a2ac1d/keras-radam-0.15.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-radam) (1.18.4)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-radam) (2.3.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (1.1.2)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (2.10.0)\n","Building wheels for collected packages: keras-radam\n","  Building wheel for keras-radam (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-radam: filename=keras_radam-0.15.0-cp36-none-any.whl size=14685 sha256=64aa25d025ade7247422ea684dc4de7883b12d058c7b8c1798f91d126a4628dd\n","  Stored in directory: /root/.cache/pip/wheels/79/a0/c0/670b0a118e8f078539fafec7bd02eba0af921f745660c7f83f\n","Successfully built keras-radam\n","Installing collected packages: keras-radam\n","Successfully installed keras-radam-0.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lqS1n-oQKoTx","colab_type":"code","colab":{}},"source":["SEQ_LEN = 128\n","BATCH_SIZE = 16\n","EPOCHS=2\n","LR=1e-5\n","\n","pretrained_path =\"bert\"\n","config_path = os.path.join(pretrained_path, 'bert_config.json')\n","checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n","vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n","\n","DATA_COLUMN = \"document\"\n","LABEL_COLUMN = \"label\"\n","\n","SETENCE1_COLUMN = 1\n","SETENCE2_COLUMN = 2\n","LABEL_COLUMN = 3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XtvjxnMLKvk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"06ca2696-1fa3-454a-a817-20c155162e2b","executionInfo":{"status":"ok","timestamp":1591070224860,"user_tz":-540,"elapsed":15081,"user":{"displayName":"김민오","photoUrl":"","userId":"03724445089918788637"}}},"source":["import os\n","!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n","\n","if \"bert\" not in os.listdir():\n","  os.makedirs(\"bert\")\n","else:\n","  pass\n","\n","import zipfile\n","import shutil\n","         \n","bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\n","bert_zip.extractall('bert')\n"," \n","bert_zip.close()\n","\n","def copytree(src, dst, symlinks=False, ignore=None):\n","    for item in os.listdir(src):\n","        s = os.path.join(src, item)\n","        d = os.path.join(dst, item)\n","        if os.path.isdir(s):\n","            shutil.copytree(s, d, symlinks, ignore)\n","        else:\n","            shutil.copy2(s, d)\n","\n","copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2020-06-02 03:56:50--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.211.128, 2607:f8b0:400c:c07::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.211.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 662903077 (632M) [application/zip]\n","Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n","\n","multi_cased_L-12_H- 100%[===================>] 632.19M   157MB/s    in 4.1s    \n","\n","2020-06-02 03:56:55 (155 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JnL7PGe7LEmQ","colab_type":"code","colab":{}},"source":["SEQ_LEN = 128\n","BATCH_SIZE = 16\n","EPOCHS=2\n","LR=1e-5\n","\n","pretrained_path =\"bert\"\n","config_path = os.path.join(pretrained_path, 'bert_config.json')\n","checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n","vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n","\n","DATA_COLUMN = \"document\"\n","LABEL_COLUMN = \"label\"\n","\n","SETENCE1_COLUMN = 1\n","SETENCE2_COLUMN = 2\n","LABEL_COLUMN = 3\n","\n","token_dict = {}\n","with codecs.open(vocab_path, 'r', 'utf8') as reader:\n","    for line in reader:\n","        token = line.strip()\n","        if \"_\" in token:\n","          token = token.replace(\"_\",\"\")\n","          token = \"##\" + token\n","        token_dict[token] = len(token_dict)\n","\n","class inherit_Tokenizer(Tokenizer):\n","  def _tokenize(self, text):\n","        if not self._cased:\n","            text = text\n","            \n","            text = text.lower()\n","        spaced = ''\n","        for ch in text:\n","            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n","                spaced += ' ' + ch + ' '\n","            elif self._is_space(ch):\n","                spaced += ' '\n","            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n","                continue\n","            else:\n","                spaced += ch\n","        tokens = []\n","        for word in spaced.strip().split():\n","            tokens += self._word_piece_tokenize(word)\n","        return tokens\n","\n","tokenizer = inherit_Tokenizer(token_dict)\n","\n","def convert_data(data_df):\n","    global tokenizer\n","    indices, targets = [], []\n","    for i in tqdm(range(len(data_df))):\n","        ids, segment = tokenizer.encode(data_df[SETENCE1_COLUMN][i], data_df[SETENCE2_COLUMN][i], max_len=SEQ_LEN)\n","        indices.append(ids)\n","        targets.append(data_df[LABEL_COLUMN][i])\n","    items = list(zip(indices, targets))\n","    \n","    indices, targets = zip(*items)\n","    indices = np.array(indices)\n","    return [indices, np.zeros_like(indices)], np.array(targets)\n","\n","def load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    \n","    \n","    data_df[SETENCE1_COLUMN] = data_df[SETENCE1_COLUMN].astype(str)\n","    data_df[SETENCE2_COLUMN] = data_df[SETENCE2_COLUMN].astype(str)\n","\n","\n","    data_x, data_y = convert_data(data_df)\n","\n","    return data_x, data_y\n","\n","def sentence_convert_data(data):\n","    global tokenizer\n","    indices = []\n","    for i in tqdm(range(len(data))):\n","        print(tokenizer.tokenize(data[i]))\n","        ids, segments = tokenizer.encode(data[i], max_len=SEQ_LEN)\n","        indices.append(ids)\n","        \n","    items = indices\n","    indices = np.array(indices)\n","    return [indices, np.zeros_like(indices)]\n","\n","def sentence_load_data(sentences):#sentence는 List로 받는다\n","           \n","    data_x = sentence_convert_data(sentences)\n","\n","    return data_x\n","\n","layer_num = 12\n","model = load_trained_model_from_checkpoint(\n","    config_path,\n","    checkpoint_path,\n","    training=True,\n","    trainable=True,\n","    seq_len=SEQ_LEN,)\n","\n","def get_bert_finetuning_model(model):\n","  inputs = model.inputs[:2]\n","  dense = model.layers[-3].output\n","  outputs = keras.layers.Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n","                              name = 'real_output')(dense)\n","\n","\n","\n","  bert_model = keras.models.Model(inputs, outputs)\n","  bert_model.compile(\n","      optimizer=RAdam(learning_rate=0.00001, weight_decay=0.0025),\n","      loss='binary_crossentropy',\n","      metrics=['accuracy'])\n","  \n","  return bert_model\n","\n","bert_model = get_bert_finetuning_model(model)\n","#bert_model.load_weights(path+\"/bert.h5\")\n","bert_model.load_weights(path+\"/bert.h8\")\n","\n","def predict_convert_data(data_df):\n","    global tokenizer\n","    indices = []\n","    for i in tqdm(range(len(data_df))):\n","        ids, segment = tokenizer.encode(data_df[SETENCE1_COLUMN][i], data_df[SETENCE2_COLUMN][i], max_len=SEQ_LEN)\n","        indices.append(ids)\n","        \n","    items = indices\n","    \n","    \n","    indices = np.array(indices)\n","    return [indices, np.zeros_like(indices)]\n","\n","def predict_load_data(x): #Pandas Dataframe을 인풋으로 받는다\n","    data_df = x\n","    \n","    \n","    data_df[SETENCE1_COLUMN] = data_df[SETENCE1_COLUMN].astype(str)\n","    data_df[SETENCE2_COLUMN] = data_df[SETENCE2_COLUMN].astype(str)\n","\n","\n","    data_x = predict_convert_data(data_df)\n","\n","    return data_x\n","\n","def sentence_convert_data(sentence1, sentece2):\n","    global tokenizer\n","    indices = []\n","    ids, segments = tokenizer.encode(sentence1, sentece2, max_len=SEQ_LEN)\n","    indices.append(ids)\n","        \n","    items = indices\n","    indices = np.array(indices)\n","    return [indices, np.zeros_like(indices)]\n","\n","def evaluation_predict(sentence1, sentece2):\n","    data_x = sentence_convert_data(sentence1, sentece2)\n","    print(data_x)\n","    predict = bert_model.predict(data_x)\n","    print(predict)\n","    predict_answer = np.round(np.ravel(predict), 0).item()\n","    \n","    if predict_answer == 0:\n","      print(\"다른 의미입니다.\")\n","    elif predict_answer == 1:\n","      print(\"같은 의미입니다.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOICCmApK_1K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"c3f55384-3f46-42e0-863d-cbbe879b5cc8","executionInfo":{"status":"ok","timestamp":1591071029941,"user_tz":-540,"elapsed":940,"user":{"displayName":"김민오","photoUrl":"","userId":"03724445089918788637"}}},"source":["evaluation_predict(\"운영체제는 소프트웨어다\", \"운영체제는 소프트웨어가 아니다\")"],"execution_count":29,"outputs":[{"output_type":"stream","text":["[array([[   101,  91988,  29683,  87164,   9448,  90927, 119173,  12965,\n","         11903,    102,  91988,  29683,  87164,   9448,  90927, 119173,\n","         12965,  11287,   9519,  48345,    102,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n","[[0.7992859]]\n","같은 의미입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UofHckQLKwwV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9VDDJ7xGK0UQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zm-FlnKLL_ad","colab_type":"code","colab":{}},"source":["\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEvRfyE4LpXl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNShUKH0LywX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":582},"outputId":"3d02b16d-686b-4a75-e346-d6c4cf49cff4","executionInfo":{"status":"error","timestamp":1591070524544,"user_tz":-540,"elapsed":655,"user":{"displayName":"김민오","photoUrl":"","userId":"03724445089918788637"}}},"source":[""],"execution_count":20,"outputs":[{"output_type":"stream","text":["[array([[None, None, None, None, None, None, None, None, None, None, None,\n","        None, None, None, None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=object), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","      dtype=object)]\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-f9a2a66e278f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"테스트\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"사과는 과일이 아니다\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-f907c91d977f>\u001b[0m in \u001b[0;36mevaluation_predict\u001b[0;34m(sentence1, sentece2)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_convert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentece2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredict_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,0] = -2147483648 is not in [0, 119547)\n\t [[{{node Embedding-Token_1/embedding_lookup}}]]"]}]},{"cell_type":"code","metadata":{"id":"xs8sf7yPL1Yc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}